{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16c0dffb-a68f-44bd-9e59-64b098de13f0",
   "metadata": {},
   "source": [
    "# Log\n",
    "This page records ideas and notes chronologically. An attempt will be made to avoid making any edits other than fixing formatting or typos.\n",
    "\n",
    "\n",
    "### 2022-09-27\n",
    "Decided to start writing down notes. \n",
    "\n",
    "At this point, I have a model that is reasonably good at predicting spikes for a single cell, given the stimulus and the spike history. This is using the distance field model output. \n",
    "\n",
    "I'll spend a bit of time today seeing how small I can make the model performance degrades. Mainly I'll try reducing channel and layer counts. I don't want to be needlessly carrying around a heavy model into the next few experiments.\n",
    "\n",
    "\n",
    "I tried out the experimentation automation tool, Guild AI; however, it was very finicky and configuration file heavy. It was especially hard to get relative paths working. In the end, I figured a manual approach would be more satisfying."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c27dd05-146b-40bd-8285-9db4139d8512",
   "metadata": {},
   "source": [
    "### 2022-09-29\n",
    "From experiment 1.1.1-1.1.3, the distance field output seems to becoming more capable of pinpointing spikes with increased channel count and number of layers. The 50ms correlation calculation from the evaluation notebook increases 1.1.1 < 1.1.2 < 1.1.3. This is despite both the best-loss for the validation set showing the opposite (1.1.3 < 1.1.2 < 1.1.1). Notably too is the discrepancy between the notebook calculated correlation and the Trainable one. The notebook calculation is often tweaked. Would be good to take more effort to keep them in sync. Another possible explanation could be the presence of a weight regularization term being included in the loss. I'm using the weight regularization term of the PyTorch optimizer; however, I was under the impression that any effects of this would not appear in the loss term. A little test shows that this is in fact the behaviour (no weight regularization term in the loss):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77895ad5-199e-41d6-80b7-1e830f0b7f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without: 5.0\n",
      "With: 5.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def weightdecay_test():\n",
    "    a = torch.Tensor([[\n",
    "        [-1,0,1,2,3]\n",
    "    ]])\n",
    "    W = torch.Tensor([\n",
    "        [1,1,1,1,1]]\n",
    "    )\n",
    "    \n",
    "    def net(x):\n",
    "        y = F.linear(x, W)\n",
    "        return y\n",
    "        \n",
    "    target = torch.Tensor([[[0]]])\n",
    "    def loss_without_reg():\n",
    "        loss_fn = torch.nn.L1Loss(reduction='sum')\n",
    "        optimizer_0 = torch.optim.Adam(params=[W], lr=0.01, weight_decay=0)\n",
    "        optimizer_0.zero_grad()\n",
    "        b = net(a)\n",
    "        loss = loss_fn(b, target=target)\n",
    "        return loss\n",
    "    \n",
    "    def loss_with_reg():\n",
    "        loss_fn = torch.nn.L1Loss(reduction='sum')\n",
    "        optimizer_0 = torch.optim.Adam(params=[W], lr=0.01, weight_decay=10)\n",
    "        optimizer_0.zero_grad()\n",
    "        b = net(a)\n",
    "        loss = loss_fn(b, target=target)\n",
    "        return loss\n",
    "   \n",
    "    print(f'Without: {loss_without_reg()}')\n",
    "    print(f'With: {loss_with_reg()}')\n",
    "    \n",
    "weightdecay_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581833f8-da3a-468c-a831-5808a21d4790",
   "metadata": {},
   "source": [
    "So what else could be causing the bigger models to have higher validation loss, yet have a better looking distance field output? Maybe the answer is that \"better looking\" is my subjective guess, and a guess based on a very small portion of the data. But the subjective guess shouldn't be ignored, as it is proximate to easier by-eye spike inference, and the current inference scripts are heavily inspired by by-eye approaches like looking for local minima and not trying to calculate the maximum likelihood solution via energy minimization. It's possible that the smaller models have a better distance field output, but the smoothness makes inference harder when using hacky inference scripts, which the current inference scripts definitely are. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
