{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a5f619-4ef6-4e00-a6d0-1476e9913259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import retinapy\n",
    "import retinapy.spikeprediction as sp\n",
    "import retinapy.mea as mea\n",
    "import numpy as np\n",
    "import torch\n",
    "import pathlib\n",
    "import matplotlib as mpl\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.subplots as subplots\n",
    "from collections import defaultdict\n",
    "import retinapy.spikedistancefield as sdf\n",
    "import torch.nn.functional as F\n",
    "import scipy\n",
    "import pathlib\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import math\n",
    "import pandas as pd\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a855bb-7bd4-47e3-8202-4078e1229724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(ds, idx):\n",
    "    \"\"\"Don't forget how to get the samples as tensor tuples.\"\"\"\n",
    "    sample = torch.utils.data.dataloader.default_collate([ds[idx]])\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18aff36-4661-40f4-8c51-7a052ca1d5d9",
   "metadata": {},
   "source": [
    "## Figures and stats, some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0959c1-bc13-40df-84ca-f58e741ded23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_plot(kernel, kernel_pad, bin_duration_ms):\n",
    "    colormap = pd.DataFrame({\n",
    "        'names':['Red', 'Green', 'UV', 'Blue', 'Stim'],\n",
    "        'display_hex':['#ff0a0a', '#0aff0a', '#0a0aff', '#303030', '#0a0a0a']})\n",
    "    fig = go.Figure()\n",
    "    xs = (np.arange(kernel.shape[0]) - kernel.shape[0] + kernel_pad) * bin_duration_ms\n",
    "    #fig.add_vline(x=-100, line_width=2, line_dash='dot', line_color='grey',\n",
    "    #              annotation_text='-100ms', annotation_position='bottom right')\n",
    "    for c in range(4):\n",
    "        fig.add_trace(go.Scatter(x=xs, \n",
    "                                 y=kernel[:,c], \n",
    "                                 line_color=colormap.loc[c]['display_hex'], \n",
    "                                 mode='lines'))\n",
    "    fig.update_layout(autosize=False,\n",
    "                      height=400,\n",
    "                      width=800,\n",
    "                      margin=dict(l=1, r=1, b=1, t=25, pad=10),\n",
    "                      yaxis_fixedrange=True,\n",
    "                      showlegend=False,\n",
    "                      title='Kernel',\n",
    "                      title_x=0.5,\n",
    "                      title_pad=dict(l=1, r=1, b=10, t=10),\n",
    "                      xaxis={'title':'time (ms), with spike at 0'},\n",
    "                      yaxis={'title':'Stimulus', 'range': [0,1]} )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03ad475-7938-4970-8b09-ebeedc929a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(pred, actual):\n",
    "    p,y  = pred, actual\n",
    "    print(p.shape, y.shape)\n",
    "    pearson_corr = scipy.stats.pearsonr(p, y)[0]\n",
    "    pr = np.round(p)\n",
    "    acc = np.mean(pr == y)\n",
    "    cm = sklearn.metrics.confusion_matrix(y, pr)\n",
    "    TN = cm[0][0]\n",
    "    FN = cm[1][0]\n",
    "    TP = cm[1][1]\n",
    "    FP = cm[0][1]\n",
    "    acc_2 = (TP + TN) / (TP + TN + FN + FP)\n",
    "    assert acc_2 == acc\n",
    "    recall = TP / (TP + FN)    # Out of all true 1s, how many are predicted as 1?\n",
    "    precision = TP / (TP + FP) # Given a 1 prediction, what is the chance that it was a 1?\n",
    "    print(f'pred 1s: {TP+FP}, actual 1s: {TP+FN}')\n",
    "    print(f'acc: {acc:.4f}, pearson_corr: {pearson_corr:.4f}, '\n",
    "          f'recall: {recall:.4f}, precision: {precision:.4f}')\n",
    "    print(f'sklearn report:\\n{sklearn.metrics.classification_report(y, pr)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac7aa6b-7f64-4cb5-b864-31f2be20fde4",
   "metadata": {},
   "source": [
    "## Load data and trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0974a3e0-5b59-4566-b8ff-7b4525be336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus_pattern_path = \"../data/ff_noise.h5\"\n",
    "stimulus_rec_path = \"../data/ff_recorded_noise.pickle\"\n",
    "response_path = \"../data/ff_spike_response.pickle\"\n",
    "rec_name = \"Chicken_17_08_21_Phase_00\"\n",
    "\n",
    "rec = mea.single_3brain_recording(\n",
    "    rec_name,\n",
    "    mea.load_stimulus_pattern(stimulus_pattern_path),\n",
    "    mea.load_recorded_stimulus(stimulus_rec_path),\n",
    "    mea.load_response(response_path),\n",
    "    include_clusters={21},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e52cfe-f27f-4e63-ad40-69168b340529",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = pathlib.Path(\"../\")\n",
    "out_dir = project_root / \"out/demo/transformer_21/0\"\n",
    "out_dir = project_root / \"out/demo/transformer_21_9ds/1\"\n",
    "model_ckpt = out_dir / \"Transformer-18ds_992in/checkpoint_best_loss.pth\"\n",
    "model_ckpt = out_dir / \"Transformer-9ds_3174in/checkpoint_best_loss.pth\"\n",
    "arg_file = out_dir / \"args.yaml\"\n",
    "assert pathlib.Path(model_ckpt).resolve().exists()\n",
    "assert pathlib.Path(arg_file).resolve().exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17018f12-3704-4b30-9c6c-feca1290b0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and model.\n",
    "input_len = 3174\n",
    "output_len = 400 \n",
    "downsample = 9\n",
    "opt = sp.args_from_yaml(arg_file)\n",
    "opt.stride = 1\n",
    "trainable = sp.TransformerTGroup.create_trainable([rec], sp.Configuration(downsample, input_len, output_len), \n",
    "                                          opt=opt)\n",
    "retinapy.models.load_model(trainable.model, model_ckpt)\n",
    "trainable.model.eval()\n",
    "trainable.model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6acc85-b9ef-46c7-80ec-ab4f0b9d6ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = trainable.test_ds.datasets[0]\n",
    "snippet_shape = test_ds[0]['snippet'].T.shape\n",
    "print(snippet_shape)\n",
    "snippet_len = snippet_shape[0]\n",
    "print(f'Snippet shape: {snippet_shape}, input_len: {input_len}, output_len: {output_len}')\n",
    "assert snippet_shape[0] == (input_len + output_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c5c698-540d-40e0-8592-8ee9c2d8d028",
   "metadata": {},
   "source": [
    "## 1. Kernel via STA approach\n",
    "The traditional STA approach to calculate kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2f1666-c745-405c-890c-ac0a1f2c9428",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " |---stimulus-----------|\n",
    " |-------------|-target-|\n",
    " |-------------|--400---|\n",
    " |-------------|-|pred|-|\n",
    "0|         3174| |    | |\n",
    "                 3    3  \n",
    "                 2    4\n",
    "                 7    7\n",
    "                 4    4\n",
    "\n",
    "\"\"\"\n",
    "def whole_distfield(trainable, pred_len=200, pred_start=100, snippet_pad=600):\n",
    "    with torch.no_grad():\n",
    "        num_overlap = 2\n",
    "        assert pred_len % num_overlap == 0\n",
    "        step = pred_len // num_overlap\n",
    "        _slice = (pred_start, pred_start + pred_len)\n",
    "        summed = torch.zeros(size=(len(trainable.test_ds)+pred_len,), device='cpu')\n",
    "        for i in range(0, len(trainable.test_ds), step):\n",
    "            sample = get_sample(trainable.test_ds, i)\n",
    "            model_out, _ = trainable.forward(sample)\n",
    "            summed[i:i+pred_len] = model_out[0,pred_start:pred_start + pred_len].cpu()\n",
    "        summed /= num_overlap\n",
    "    # Urh. There is a prefix due to spike prediction happening after the 3174 bins that\n",
    "    # are used for the input. Add in the offset for the fact that we are not taking the\n",
    "    # whole model output, but a slice starting at pred_start. \n",
    "    prefix_len = test_ds.mask_slice.start + pred_start - 1\n",
    "    suffix_len = output_len - (pred_len + pred_start) + snippet_pad\n",
    "    #  150ms. Aiming for the average distance.\n",
    "    pad_value_as_dist = 300 \n",
    "    pad_value = trainable.distfield_to_nn_output(torch.Tensor([pad_value_as_dist])).item()\n",
    "    dist = np.concatenate([\n",
    "        np.full(shape=(prefix_len,), fill_value=pad_value),\n",
    "        summed.numpy(),\n",
    "        np.full(shape=(suffix_len,), fill_value=pad_value),\n",
    "    ])\n",
    "    return dist\n",
    "\n",
    "def spike_prob(distfield):\n",
    "    upshift = 0.5\n",
    "    prob = torch.clamp(-distfield + upshift, min=0, max=3)\n",
    "    return prob\n",
    "\n",
    "def infer_spikes(dist, dist_threshold, grad_threshold):\n",
    "    dist = _blur(dist, sigma=10.0)\n",
    "    is_near_spike = (dist < dist_threshold)\n",
    "    grad = np.gradient(dist)\n",
    "    has_low_grad = (grad < grad_threshold)\n",
    "    has_positive_grad2 = np.gradient(grad) > 0.00\n",
    "    spikes = is_near_spike & has_low_grad & has_positive_grad2\n",
    "    return spikes\n",
    "\n",
    "def infer_spikes_via_spline(dist, dist_threshold, grad_threshold, smoothing=0):\n",
    "    #spline = scipy.interpolate.InterpolatedUnivariateSpline(x=xs,  y=dist, k=4)#s=smoothing)\n",
    "    downsample = 1\n",
    "    dist_ds = scipy.signal.decimate(dist, downsample)\n",
    "    xs = np.arange(len(dist_ds))\n",
    "    spline = scipy.interpolate.UnivariateSpline(x=xs,  y=dist_ds, k=4, s=smoothing)\n",
    "    roots_idxs = np.round(spline.derivative().roots()).astype(int)\n",
    "    grad2_at_roots = spline.derivative().derivative()(roots_idxs)\n",
    "    dist_at_roots = spline(roots_idxs)\n",
    "    minima_idxs = roots_idxs[np.logical_and(grad2_at_roots > 0, \n",
    "                                            dist_at_roots < dist_threshold)] * downsample\n",
    "    minima = np.zeros_like(dist)\n",
    "    minima[minima_idxs] = 1\n",
    "    return minima, spline\n",
    "    grad = spline.derivative(n=1)(xs)\n",
    "    grad2 = spline.derivative(n=2)(xs)\n",
    "    is_near_spike = (spline(xs) < dist_threshold)\n",
    "    is_near_local_extremum = (grad < grad_threshold)\n",
    "    is_min = (grad2 > 0)\n",
    "    spikes = is_near_spike & is_near_local_extremum & is_min\n",
    "    return spikes, spline\n",
    "\n",
    "def _blur(x, sigma):\n",
    "    return scipy.ndimage.gaussian_filter1d(x, sigma=sigma, axis=0, mode='constant', cval=0.0)\n",
    "\n",
    "\n",
    "\n",
    "def sta(trainable):\n",
    "    test_ds = trainable.test_ds.datasets[0]\n",
    "    threshold = 0\n",
    "    pred_len = 200\n",
    "    pred_start = 100\n",
    "    snippet_pad = test_ds.pad\n",
    "    print(f'len(test_ds): {len(test_ds)}')\n",
    "    print(len(test_ds) + 1000 + input_len)\n",
    "    print(len(test_ds.ds.recording.stimulus))\n",
    "    #assert len(test_ds)+1000+input_len-1 == len(test_ds.ds.recording.stimulus)\n",
    "    distfield = whole_distfield(trainable, pred_len, pred_start, snippet_pad)\n",
    "\n",
    "    # Predicted spikes.\n",
    "    pred_spikes, spline = infer_spikes_via_spline(distfield, dist_threshold=-0.9, grad_threshold=0.2)\n",
    "    num_color_channels = 4\n",
    "    #Actual spikes\n",
    "    actual_spikes = test_ds.ds.recording.spikes[:,0]\n",
    "    \n",
    "    print_stats(pred=pred_spikes, actual=actual_spikes)\n",
    "    pred_spike_idxs = np.squeeze(np.nonzero(pred_spikes))\n",
    "    actual_spike_idxs = np.squeeze(np.nonzero(actual_spikes))\n",
    "    stim =  test_ds.ds.recording.stimulus\n",
    "    pred_spike_snippets = retinapy.mea.spike_snippets(stim, pred_spike_idxs, total_len=input_len+output_len, post_spike_len=output_len)\n",
    "    actual_spike_snippets = retinapy.mea.spike_snippets(stim, actual_spike_idxs, total_len=input_len+output_len, post_spike_len=output_len)\n",
    "    pred_kernel = np.mean(pred_spike_snippets, axis=0)\n",
    "    actual_kernel = np.mean(actual_spike_snippets, axis=0)\n",
    "    return pred_kernel, actual_kernel, distfield, pred_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8447b3b1-4ebe-45e3-8650-8e5bfd172b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_kernel, actual_kernel, distfield, pred_spikes = sta(trainable)\n",
    "pred_kernel_1ms = scipy.signal.decimate(pred_kernel, 2, axis=0)\n",
    "actual_kernel_1ms = scipy.signal.decimate(actual_kernel, 2, axis=0)\n",
    "kernel_pad_1ms = 200\n",
    "bin_duration_ms = 1.008"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e00749-9d89-4f92-9ed7-f08a26a82b25",
   "metadata": {},
   "source": [
    "### 1.1 Results (the kernel)\n",
    "The steps to calculate this kernel are:\n",
    "\n",
    "    1. Use pretrained model to generate a distance field for the whole training data snippet.\n",
    "    2. Try and guess the location of spikes based on the distance field created in 1. This is a very\n",
    "        rudimentary heuristic approach to estimating the maximum likelihood spikes.\n",
    "    3. Run the standard spike train analysis on the spikes from 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c8e41d-edc1-45a4-86fa-c0863f4830d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = kernel_plot(pred_kernel_1ms, kernel_pad=kernel_pad_1ms, bin_duration_ms=bin_duration_ms)\n",
    "fig.write_image('../out/kernel_fake.svg')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c053ee50-1664-4462-a94c-942daacbedaa",
   "metadata": {},
   "source": [
    "### 1.2 True STA kernel (from recorded spikes)\n",
    "For comparison, here is the STA kernel calculated from recorded spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9672a0-aa85-4e57-bb29-47239bd7b049",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = kernel_plot(actual_kernel_1ms, kernel_pad=kernel_pad_1ms, bin_duration_ms=bin_duration_ms)\n",
    "fig.write_image('../out/kernel_real.svg')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8d1f07-e3ae-4ba6-9b9d-30a383f928f6",
   "metadata": {},
   "source": [
    "### 1.3 Spike inference. Quality check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6e7fa7-00ac-42b8-af8b-ca1e4c76f024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _debug_spike_inference(pred_distfield, actual_distfield, start_ms=0, duration_ms=2000):\n",
    "    downsample = 2\n",
    "    pred_spikes, spline = infer_spikes_via_spline(pred_distfield, dist_threshold=-0.9, \n",
    "                                                  grad_threshold=0.1)\n",
    "    xs_double_res = np.arange(duration_ms*downsample) + start_ms*downsample\n",
    "    xs = np.arange(duration_ms) + start_ms\n",
    "    spline_1ms = scipy.signal.decimate(spline(xs_double_res), downsample)\n",
    "    pred_df_1ms = scipy.signal.decimate(pred_distfield, downsample, axis=0)\n",
    "    actual_df_1ms = scipy.signal.decimate(actual_distfield, downsample, axis=0)\n",
    "    spike_idxs = np.round(np.squeeze(np.nonzero(pred_spikes)).astype(float) / downsample)\n",
    "    scatter_p = go.Scatter(x=xs, y=pred_df_1ms[start_ms:start_ms+duration_ms], \n",
    "                          name=\"Pred log(dist)\")\n",
    "    scatter_y = go.Scatter(x=xs, y=actual_df_1ms[start_ms:start_ms+duration_ms],\n",
    "                          name=\"Actual log(dist)\")\n",
    "    scatter_s = go.Scatter(x=xs, y=spline_1ms,\n",
    "                           name=\"Spline approx\")\n",
    "    fig = subplots.make_subplots(rows=3, cols=1, shared_xaxes=True) \n",
    "    fig.add_trace(scatter_p, row=1, col=1)\n",
    "    fig.add_trace(scatter_y, row=2, col=1)\n",
    "    fig.add_trace(scatter_s, row=3, col=1)\n",
    "    fig.update_layout({\n",
    "        \"height\": 600,\n",
    "        \"width\": 800})\n",
    "    spikes_in_xs = spike_idxs[np.logical_and(spike_idxs > start_ms, spike_idxs < start_ms + duration_ms)]\n",
    "    #spikes_in_xs = np.squeeze(np.nonzero(pred_spikes[start_ms:start_ms+duration_ms])) + start_ms\n",
    "    print(len(spikes_in_xs))\n",
    "    if len(spikes_in_xs) < 100:\n",
    "        for s in spikes_in_xs:\n",
    "            fig.add_vline(x=s, line_width=0.8, line_dash='dot', line_color='grey', row=1, col=1)\n",
    "    fig.show()\n",
    "    fig.write_image(\"../out/spikes1.svg\")\n",
    "\n",
    "def debug_spike_inference(trainable, start_ms):\n",
    "    pred_len = 200\n",
    "    pred_start = 100\n",
    "    snippet_pad = 600\n",
    "    pred_distfield = whole_distfield(trainable, pred_len, pred_start, snippet_pad)\n",
    "    max_dist = 600\n",
    "    actual_distfield = sdf.distance_field(test_ds.ds.recording.spikes[:,0], max_dist)\n",
    "    actual_distfield = trainable.distfield_to_nn_output(torch.Tensor(actual_distfield)).numpy()\n",
    "    _debug_spike_inference(pred_distfield, actual_distfield, start_ms)\n",
    "    \n",
    "debug_spike_inference(trainable, start_ms=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067a02bc-2133-4db6-a8bf-68e4fac95234",
   "metadata": {},
   "source": [
    "## 2. Kernel vias optimized input\n",
    "Calculate the pseudo kernel by optimizing the input stimulus for a given model output.\n",
    "\n",
    "Some inspiration for the training loop came from: https://github.com/utkuozbulak/pytorch-cnn-visualizations/blob/master/src/cnn_layer_visualization.py.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c62ffff-1bf4-4671-966f-00401fbcce69",
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = pd.DataFrame({\n",
    "    'names':['Red', 'Green', 'UV', 'Blue', 'Stim'],\n",
    "    'display_hex':['#ff0a0a', '#0aff0a', '#0a0aff', '#303030', '#0a0a0a']})\n",
    "\n",
    "def kernel_plot2(kernel, snippet_pad, bin_duration_ms):\n",
    "    fig = go.Figure()\n",
    "    xs = (np.arange(kernel.shape[0]) - kernel.shape[0]) * bin_duration_ms\n",
    "    # Shift the x-axis to have zero in the middle.\n",
    "    xs += snippet_pad\n",
    "    #fig.add_vline(x=-100, line_width=2, line_dash='dot', line_color='grey',\n",
    "    #              annotation_text='-100ms', annotation_position='bottom right')\n",
    "    for c in range(4):\n",
    "        fig.add_trace(go.Scatter(x=xs, \n",
    "                                 y=kernel[:,c], \n",
    "                                 line_color=colormap.loc[c]['display_hex'], \n",
    "                                 mode='lines'))\n",
    "    fig.update_layout(autosize=False,\n",
    "                      height=400,\n",
    "                      width=800,\n",
    "                      margin=dict(l=1, r=1, b=1, t=25, pad=1),\n",
    "                      yaxis_fixedrange=True,\n",
    "                      showlegend=False,\n",
    "                      title='Kernel',\n",
    "                      title_x=0.5,\n",
    "                      title_pad=dict(l=1, r=1, b=10, t=1),\n",
    "                      xaxis={'title':'time (ms), with spike at 0'},\n",
    "                      yaxis={'title':'Stimulus', 'range': [-2,3]} )\n",
    "    return fig\n",
    "\n",
    "\n",
    "def display_dist(model_out, y, downsample=2, loss_slice=None):\n",
    "    y = scipy.signal.decimate(y, downsample)\n",
    "    model_out = scipy.signal.decimate(model_out, downsample)\n",
    "    xs = np.arange(len(y))\n",
    "    fig = go.Figure()\n",
    "    scatter_model = go.Scatter(x=xs, y=model_out, name='Pred', line_color='gray')\n",
    "    fig.add_trace(scatter_model)\n",
    "    scatter_y = go.Scatter(x=xs, y=y, name='Actual', line_color='red')\n",
    "    fig.add_trace(scatter_y)\n",
    "    if loss_slice:\n",
    "        fig.add_vline(x=loss_slice.start//downsample, line_width=0.8, \n",
    "                      line_dash='dot', line_color='grey', row=1, col=1)\n",
    "        fig.add_vline(x=loss_slice.stop//downsample, line_width=0.8, \n",
    "                      line_dash='dot', line_color='grey', row=1, col=1)\n",
    "    fig.update_layout(height=400,\n",
    "                      width=800,\n",
    "                      title='Log distance field. Model (gray), actual (red)')\n",
    "    return fig\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "def calc_kernel(trainable, num_steps=10, \n",
    "                spikes=None,\n",
    "                loss_slice=None,\n",
    "                mag_weight=0.01, freq_weight=0.01, cutoff_freq=None):\n",
    "    trainable.model.eval()\n",
    "    num_shifts = 5\n",
    "    base_len = test_ds[0]['snippet'].shape[1] # 3574\n",
    "    color_channels = 4\n",
    "    total_stim_len = base_len + num_shifts\n",
    "    input_stim = torch.normal(mean=0, std=2, size=(color_channels, total_stim_len), requires_grad=True, dtype=torch.float32, device='cuda')\n",
    "    device = input_stim.device\n",
    "    assert input_stim.shape[1] == base_len + num_shifts\n",
    "    # Set the spike channel to zero.\n",
    "    spike_history = torch.zeros(size=(1, total_stim_len), \n",
    "                                #requires_grad=True, \n",
    "                                device=device)\n",
    "    spike_history[0, 3174:-1] = test_ds.MASK_VALUE\n",
    "    # We can't enable grad before the stack, as it will be copied.\n",
    "    # see: https://discuss.pytorch.org/t/variable-update-after-autograd-in-torch-cat-torch-vstack-nn-zeropad2d-torch-index-select-and-torch-roll/124823\n",
    "    #input_stim = input_snippet[0,0:5,:]\n",
    "    #input_stim.requires_grad_(True)\n",
    "    optimizer = torch.optim.Adam([input_stim], lr=0.005, weight_decay=0)\n",
    "    \n",
    "    # Frequency reg.\n",
    "    freq_bins = torch.fft.rfftfreq(total_stim_len, d=1/2231.596)\n",
    "    #torch.zeros_like(freq_bins, device=device)\n",
    "    if cutoff_freq:\n",
    "        freq_penalty = torch.zeros_like(freq_bins, device=device)\n",
    "        freq_penalty[torch.where(freq_bins > cutoff_freq)] = freq_weight\n",
    "    else:\n",
    "        freq_penalty = torch.arange(len(freq_bins), device=device)/len(freq_bins)\n",
    "    \n",
    "    \n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    def target_nn_output(spike_indicies, shift, length=400):\n",
    "        spikes = np.zeros((length,))\n",
    "        spikes[spike_indicies+shift] = 1\n",
    "        df = retinapy.spikedistancefield.distance_field(spikes, default_distance=600)\n",
    "        res = trainable.distfield_to_nn_output(torch.Tensor(df))\n",
    "        res = torch.unsqueeze(res, 0).cuda()\n",
    "        return res\n",
    "    \n",
    "    # Precompute the targets, as they don't change.\n",
    "    targets = [target_nn_output(np.array(spikes), shift=-d, length=400) for d in range(num_shifts)]\n",
    "    targets = torch.vstack(targets)\n",
    "    for i in range(0, num_steps):\n",
    "        optimizer.zero_grad()\n",
    "        input_snippet = torch.stack(\n",
    "            [torch.vstack([input_stim, torch.abs(spike_history)])[:,start:start+base_len] for start in range(num_shifts)])\n",
    "        rec_id, c_id = (torch.Tensor([0,0,0,0,0]).long().cuda(), torch.Tensor([0,0,0,0,0]).long().cuda())\n",
    "        model_out,_,_ = trainable.model(input_snippet, rec_id, c_id)\n",
    "        model_loss = torch.sum(torch.norm((model_out - targets)[:,loss_slice], p=2, dim=1))\n",
    "        loss = model_loss\n",
    "        mag_loss = mag_weight * torch.sum(torch.norm(input_stim, p=1, dim=1))\n",
    "        freq_loss = torch.norm(torch.fft.rfft(input_stim) * freq_penalty)\n",
    "        #spike_loss = torch.sum(torch.abs(spike_history))\n",
    "        #loss += spike_loss\n",
    "        loss += mag_loss \n",
    "        loss += freq_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 500 == 0:\n",
    "            print(f'Loss. mag: {mag_loss}, freq: {freq_loss}, model: {model_loss}')\n",
    "    res = input_stim[:,0:base_len].detach().cpu().numpy().T\n",
    "    return res, targets.cpu(), (input_snippet.detach(), rec_id.detach(), c_id.detach())\n",
    "\n",
    "def smooth(stimulus):\n",
    "    # Bins are 0.5ms = 2000\n",
    "    fs = 2000\n",
    "    nyq = 0.5 * fs\n",
    "    cutoff = 30\n",
    "    b, a = scipy.signal.butter(3, cutoff/nyq)\n",
    "    s = scipy.signal.filtfilt(b, a, stimulus, axis=0, method='gust')\n",
    "    return s\n",
    "    \n",
    "def smooth_bl(stimulus):\n",
    "    stimulus_ = np.expand_dims(stimulus,0)\n",
    "    stimulus[:,0:3] = cv.bilateralFilter(stimulus_[:,:,0:3], d=0, sigmaColor=1, sigmaSpace=100)\n",
    "    res = stimulus\n",
    "    res[:,3] = 0\n",
    "    return res\n",
    "\n",
    "def blur(stimulus, sigma=10.0):\n",
    "    return scipy.ndimage.gaussian_filter1d(stimulus, sigma=sigma, axis=0, mode='constant', cval=0.0)\n",
    "\n",
    "def calc_and_display_opt_input():\n",
    "    k, targets, last_inputs = calc_kernel(trainable, num_steps=1000)\n",
    "    k_1ms = scipy.signal.decimate(k, 2, axis=0)\n",
    "    #k_1ms = smooth(k)\n",
    "    smooth_k = k_1ms\n",
    "    smooth_k = blur(k_1ms)\n",
    "\n",
    "    # 3574 (3174 + 400)\n",
    "    fig = kernel_plot2(smooth_k, snippet_pad=400//2, bin_duration_ms=1.008)\n",
    "    fig.show()\n",
    "    model_out = trainable.model(last_inputs).detach().cpu().numpy()\n",
    "    #fig = display_dist(model_out[0], targets[0].cpu().numpy())\n",
    "    #fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063f9d73-9770-4a36-823e-53670a7874c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 3\n",
    "ks = []\n",
    "for t in range(num_trials):\n",
    "    downsample = 2\n",
    "    spikes = np.array([50])*downsample\n",
    "    loss_slice=slice(0*downsample, 50*downsample)\n",
    "    \n",
    "    k, targets, last_inputs = calc_kernel(trainable, \n",
    "                                          num_steps=5000,\n",
    "                                          spikes=spikes,\n",
    "                                          loss_slice=loss_slice,\n",
    "                                          mag_weight=0,#0.002,\n",
    "                                          freq_weight=0,#0.02,\n",
    "                                          cutoff_freq=100)\n",
    "    \n",
    "    k_1ms = scipy.signal.decimate(k, 2, axis=0)\n",
    "    ks.append(k_1ms)\n",
    "    fig = kernel_plot2(k_1ms, snippet_pad=400//2, bin_duration_ms=1.008)\n",
    "    fig.show()\n",
    "    fig.write_image('../out/MEI_no_reg.svg')\n",
    "    model_out = trainable.model(last_inputs).detach().cpu().numpy()\n",
    "    fig = display_dist(model_out[0], targets[0].cpu().numpy(), loss_slice=loss_slice, downsample=2)\n",
    "    fig.show()\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057cc3db-33bb-4735-bbb6-78f561fc6246",
   "metadata": {},
   "source": [
    "## Smoothed kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f86b168-e70d-49cb-bc81-9edd08d87680",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ks:\n",
    "    fig = kernel_plot(blur(k, sigma=10), snippet_pad=200, bin_duration_ms=1.008)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8768d62d-950c-4c15-8ea7-7f698b12d38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6ef159-ab21-4834-b8f3-2458ee7488bf",
   "metadata": {},
   "source": [
    "## Cluster 21 STA kernel\n",
    "![Cluster 21 kernel](resources/cluster21_kernel.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205d7149-58c4-4a5a-827f-3e2affb51894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95130595-3a3a-4649-a07b-8e3595122240",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
